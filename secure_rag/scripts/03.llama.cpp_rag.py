#!/usr/bin/env python3
import requests
import json
import os
import sys

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# URL —Ç–≤–æ–µ–≥–æ RAG API —Å–µ—Ä–≤–µ—Ä–∞ (04.integration.py)
RAG_API_URL = "http://localhost:9000/search"
# URL —Ç–≤–æ–µ–≥–æ llama-server (–æ–±—ã—á–Ω–æ 8080)
LLAMA_SERVER_URL = "http://localhost:8080/completion"
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç RAG
K_RETRIEVED_CHUNKS = 3
# –ú–æ–¥–µ–ª—å LLM, –∫–æ—Ç–æ—Ä—É—é —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –≤ llama-server
# –£–±–µ–¥–∏—Å—å, —á—Ç–æ —ç—Ç–æ –∏–º—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –≤ llama-server
LLM_MODEL_NAME = "saiga_yandexgpt_8b.Q4_K_M.gguf" # –ü—Ä–∏–º–µ—Ä: –∑–∞–º–µ–Ω–∏ –Ω–∞ —Ç–≤–æ—é –º–æ–¥–µ–ª—å

# --- –§—É–Ω–∫—Ü–∏–∏ ---

def get_rag_context(query: str) -> list:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –Ω–∞ RAG API —Å–µ—Ä–≤–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    print(f"\nüîé –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –Ω–∞ RAG API: '{query}'...")
    try:
        response = requests.get(RAG_API_URL, params={"query": query, "k": K_RETRIEVED_CHUNKS})
        response.raise_for_status() # –í—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ HTTP (4xx –∏–ª–∏ 5xx)
        data = response.json()
        
        if data and "results" in data:
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(data['results'])} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
            return data["results"]
        else:
            print("‚ö† RAG API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")
            return []
    except requests.exceptions.ConnectionError:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ RAG API —Å–µ—Ä–≤–µ—Ä—É –ø–æ –∞–¥—Ä–µ—Å—É {RAG_API_URL}.")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ '04.integration.py' –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω.")
        return []
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ RAG API: {e}")
        return []

def generate_llm_response(prompt: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–º–ø—Ç –Ω–∞ llama-server –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
    """
    print(f"\nüß† –û—Ç–ø—Ä–∞–≤–ª—è—é –ø—Ä–æ–º–ø—Ç –Ω–∞ llama-server...")
    headers = {"Content-Type": "application/json"}
    payload = {
        "prompt": prompt,
        "n_predict": 2048, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        "temperature": 0.7,
        "stop": ["\nUser:", "\n###", "<|im_end|>", "<|endoftext|>"], # –û—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è —á–∞—Ç–∞
        "model": LLM_MODEL_NAME # –£–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ llama-server –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
    }
    
    try:
        response = requests.post(LLAMA_SERVER_URL, headers=headers, json=payload, stream=False)
        response.raise_for_status()
        
        # llama-server –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –≤ JSON, –≥–¥–µ "content" —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç
        result = response.json()
        if "content" in result:
            print("‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç llama-server.")
            return result["content"].strip()
        else:
            print("‚ö† llama-server –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç 'content').")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM."
    except requests.exceptions.ConnectionError:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ llama-server –ø–æ –∞–¥—Ä–µ—Å—É {LLAMA_SERVER_URL}.")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ llama-server –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω.")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ LLM —Å–µ—Ä–≤–µ—Ä—É."
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ llama-server: {e}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ LLM: {e}"

def main():
    print("=== –ó–∞–ø—É—Å–∫ RAG-—Å–∏—Å—Ç–µ–º—ã —Å LLAMA.cpp ===")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit'.")

    while True:
        user_query = input("\n–¢–≤–æ–π –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ").strip()
        if user_query.lower() == 'exit':
            print("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã RAG-—Å–∏—Å—Ç–µ–º—ã. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not user_query:
            print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
            continue

        # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG API
        retrieved_docs = get_rag_context(user_query)

        context_text = ""
        if retrieved_docs:
            context_text = "\n\n### –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n"
            for i, doc in enumerate(retrieved_docs):
                context_text += f"–î–æ–∫—É–º–µ–Ω—Ç {i+1} (–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}):\n"
                context_text += doc.get("content", "") + "\n---\n"
        else:
            print("‚ö† –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ç–≤–µ—Ç LLM –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º.")

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —à–∞–±–ª–æ–Ω, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º LLM
        prompt_template = f"""
–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Ç–≤–µ—á–∞–π, —á—Ç–æ –Ω–µ –º–æ–∂–µ—à—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

{context_text}

### –í–æ–ø—Ä–æ—Å:
{user_query}

### –û—Ç–≤–µ—Ç:
"""
        # print("\n--- –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏) ---")
        # print(prompt_template)
        # print("---------------------------------------------------\n")

        # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ LLM –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        llm_response = generate_llm_response(prompt_template)
        
        print("\n--- –û—Ç–≤–µ—Ç LLM ---")
        print(llm_response)
        print("-------------------\n")

if __name__ == "__main__":
    main()

