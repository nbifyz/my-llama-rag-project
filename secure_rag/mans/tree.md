(base) user@mx:~/secure_rag
$ tree
.
├── documents
│   └── _my
│       ├── bred1.md
│       ├── degret3fault.md
│       ├── gl3.md
│       ├── glur2.md
│       ├── gluu6.md
│       └── gr5.md
├── logs
├── mans
├── scripts
│   ├── 01_ai_text_generator.sh
│   ├── 02.create_vector_db.py
│   ├── 02.test_vector_base.py
│   ├── 03.openwebui_rag.py
│   └── 04.integration.py
├── secure_rag_system.py
└── vector_db
*************************************
(base) user@mx:/opt/llama.cpp/build/bin
$ tree
.
├── libggml-base.so
├── libggml-cpu.so
├── libggml-cuda.so
├── libggml.so
├── libllama.so
├── libmtmd.so
├── llama-batched
├── llama-batched-bench
├── llama-bench
├── llama-cli
├── llama-convert-llama2c-to-ggml
├── llama-cvector-generator
├── llama-embedding
├── llama-eval-callback
├── llama-export-lora
├── llama-finetune
├── llama-gemma3-cli
├── llama-gen-docs
├── llama-gguf
├── llama-gguf-hash
├── llama-gguf-split
├── llama-gritlm
├── llama-imatrix
├── llama-llava-cli
├── llama-lookahead
├── llama-lookup
├── llama-lookup-create
├── llama-lookup-merge
├── llama-lookup-stats
├── llama-minicpmv-cli
├── llama-mtmd-cli
├── llama-parallel
├── llama-passkey
├── llama-perplexity
├── llama-q8dot
├── llama-quantize
├── llama-qwen2vl-cli
├── llama-retrieval
├── llama-run
├── llama-save-load-state
├── llama-server
├── llama-simple
├── llama-simple-chat
├── llama-speculative
├── llama-speculative-simple
├── llama-tokenize
├── llama-tts
├── llama-vdot
├── test-arg-parser
├── test-autorelease
├── test-backend-ops
├── test-barrier
├── test-c
├── test-chat
├── test-chat-parser
├── test-chat-template
├── test-gbnf-validator
├── test-gguf
├── test-grammar-integration
├── test-grammar-parser
├── test-json-partial
├── test-json-schema-to-grammar
├── test-llama-grammar
├── test-log
├── test-model-load-cancel
├── test-mtmd-c-api
├── test-quantize-fns
├── test-quantize-perf
├── test-quantize-stats
├── test-regex-partial
├── test-rope
├── test-sampling
├── test-thread-safety
├── test-tokenizer-0
├── test-tokenizer-1-bpe
└── test-tokenizer-1-spm

****************************

(base) user@mx:~/models
$ tree
.
├── embeded
│   └── paraphrase-multilingual-minilm-l12-v2.Q6_K.gguf
├── embeding
│   └── BAAI-bge-m3
│       ├── 1_Pooling
│       │   └── config.json
│       ├── colbert_linear.pt
│       ├── config.json
│       ├── config_sentence_transformers.json
│       ├── imgs
│       │   ├── bm25.jpg
│       │   ├── long.jpg
│       │   ├── miracl.jpg
│       │   ├── mkqa.jpg
│       │   ├── nqa.jpg
│       │   └── others.webp
│       ├── long.jpg
│       ├── modules.json
│       ├── onnx
│       │   ├── config.json
│       │   ├── Constant_7_attr__value
│       │   ├── model.onnx
│       │   ├── model.onnx_data
│       │   ├── sentencepiece.bpe.model
│       │   ├── special_tokens_map.json
│       │   ├── tokenizer_config.json
│       │   └── tokenizer.json
│       ├── pytorch_model.bin
│       ├── README.md
│       ├── sentence_bert_config.json
│       ├── sentencepiece.bpe.model
│       ├── sparse_linear.pt
│       ├── special_tokens_map.json
│       ├── tokenizer_config.json
│       └── tokenizer.json
└── GGUF
    ├── ALIENTELLIGENCE.kcpps
    ├── ALIENTELLIGENCE_roleplaymaster-latest.gguf
    ├── bartowski_writing-roleplay-12b-Q4_K_M.gguf
    ├── cobo1.kcpps
    ├── Counterfeit-V3.0_fix_fp16.safetensors
    ├── mistral-7b-grok-Q4_K_M.gguf
    ├── nethena-mlewd-xwin-23b.Q5_K_M.gguf
    ├── qwen2.5coder_14_binstructq_8_0.gguf
    ├── qwen2.5coder_32b.gguf
    ├── qwen2.5-coder-3b-instruct-q6_k.gguf
    └── saiga_yandexgpt_8b.Q4_K_M.gguf

8 directories, 40 files


